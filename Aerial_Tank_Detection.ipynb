{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GGBNr5tZcD_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LYihjrZcEA"
      },
      "source": [
        "Tank Detection from Drone POV using TorchVision\n",
        "================================================\n",
        "For this tutorial, we will be finetuning a pre-trained [Faster\n",
        "R-CNN](https://arxiv.org/abs/1506.01497) model on a dataset of aerial-view images of tanks. We will use the dataset to illustrate how to train and finetune a model to perform object detection.\n",
        "\n",
        "Our dataset contains various images of tanks, including images from video games, tabletop models, and real-life imagery from the Ukraine-Russia war.\n",
        "In the full dataset there are 6510 training images, 685 validation images, and 295 test images; however, for today we will be working with a mini subset of the full training data to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/SOCOM-Ignite/ML_AI_Kickoff/releases/download/v1.0.0/AerialTankData.zip -P data\n",
        "!cd data && unzip AerialTankData.zip"
      ],
      "metadata": {
        "id": "w4GXwky_TbK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "image = read_image(\"/content/data/AerialTankData/valid/150630-vaux-russia-tease_qeayov_jpeg_jpg.rf.0f1530b045fd8550b96d60fce4efa269.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "m_YbqUu3la65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqUX7Tv6ZcED"
      },
      "source": [
        "\n",
        "Defining your model\n",
        "===================\n",
        "\n",
        "In this tutorial, we will be using\n",
        "[Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a\n",
        "model that predicts both bounding boxes and class scores for potential\n",
        "objects in the image.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png)\n",
        "\n",
        "\n",
        "There are two common situations where one might want to modify one of\n",
        "the available models in TorchVision Model Zoo. The first is when we want\n",
        "to start from a pre-trained model, and just finetune the last layer. The\n",
        "other is when we want to replace the backbone of the model with a\n",
        "different one (for faster predictions, for example).\n",
        "\n",
        "Let's go see how we would do one or another in the following sections.\n",
        "\n",
        "1 - Finetuning from a pretrained model\n",
        "--------------------------------------\n",
        "\n",
        "Let's suppose that you want to start from a model pre-trained on COCO\n",
        "and want to finetune it for your particular classes. Here is a possible\n",
        "way of doing it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_fmSndHZcED"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTCvoKB5ZcEE"
      },
      "source": [
        "2 - Modifying the model to add a different backbone\n",
        "===================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvFNY3mOZcEE"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# ``FasterRCNN`` needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "    featmap_names=['0'],\n",
        "    output_size=7,\n",
        "    sampling_ratio=2\n",
        ")\n",
        "\n",
        "# put the pieces together inside a Faster-RCNN model\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=2,\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbOtg43zZcEE"
      },
      "source": [
        "Object detection and instance segmentation model for PennFudan Dataset\n",
        "======================================================================\n",
        "\n",
        "In our case, we want to finetune from a pre-trained model, given that\n",
        "our dataset is very small, so we will be following approach number 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECeuwJ7VZcEF"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "\n",
        "def get_model_detection(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTW9TkZoZcEF"
      },
      "source": [
        "That's it, this will make `model` be ready to be trained and evaluated\n",
        "on your custom dataset.\n",
        "\n",
        "Putting everything together\n",
        "===========================\n",
        "\n",
        "In `references/detection/`, we have a number of helper functions to\n",
        "simplify training and evaluating detection models. Here, we will use\n",
        "`references/detection/engine.py` and `references/detection/utils.py`.\n",
        "Just download everything under `references/detection` to your folder and\n",
        "use them here. On Linux if you have `wget`, you can download them using\n",
        "below commands:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsMZzEerZcEF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPyKscWiZcEF"
      },
      "source": [
        "Since v0.15.0 torchvision provides [new Transforms\n",
        "API](https://pytorch.org/vision/stable/transforms.html) to easily write\n",
        "data augmentation pipelines for Object Detection and Segmentation tasks.\n",
        "\n",
        "Let's write some helper functions for data augmentation /\n",
        "transformation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siyVTAPkZcEF"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "import torch\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToImage())\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.coco import CocoDetection\n",
        "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
        "from coco_utils import _coco_remove_images_without_annotations\n",
        "\n",
        "\n",
        "train_dataset = CocoDetection(\n",
        "    root=\"/content/data/AerialTankData/train\",\n",
        "    annFile=\"/content/data/AerialTankData/train/_annotations.coco.mini.json\",\n",
        "    transform=get_transform(train=True),\n",
        ")\n",
        "valid_dataset = CocoDetection(\n",
        "    root=\"/content/data/AerialTankData/valid\",\n",
        "    annFile=\"/content/data/AerialTankData/valid/_annotations.coco.mini.json\",\n",
        "    transform=get_transform(train=False),\n",
        ")\n",
        "test_dataset = CocoDetection(\n",
        "    root=\"/content/data/AerialTankData/test\",\n",
        "    annFile=\"/content/data/AerialTankData/test/_annotations.coco.mini.json\",\n",
        "    transform=get_transform(train=False),\n",
        ")\n",
        "\n",
        "train_dataset = wrap_dataset_for_transforms_v2(train_dataset)\n",
        "valid_dataset = wrap_dataset_for_transforms_v2(valid_dataset)\n",
        "test_dataset = wrap_dataset_for_transforms_v2(test_dataset)\n",
        "\n",
        "train_dataset = _coco_remove_images_without_annotations(train_dataset)\n",
        "test_dataset = _coco_remove_images_without_annotations(test_dataset)\n",
        "valid_dataset = _coco_remove_images_without_annotations(valid_dataset)"
      ],
      "metadata": {
        "id": "bD6U7ltIokWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZP3JDuKZcEG"
      },
      "source": [
        "Testing `forward()` method (Optional)\n",
        "=====================================\n",
        "\n",
        "Before iterating over the dataset, it\\'s good to see what the model\n",
        "expects during training and inference time on sample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejmplJFYZcEG"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "\n",
        "model = get_model_detection(num_classes=2)\n",
        "dataset = train_dataset\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# For Training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "output = model(images, targets)  # Returns losses and detections\n",
        "print(output)\n",
        "\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)  # Returns predictions\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhmpY8HwZcEG"
      },
      "source": [
        "Let's now write the main function which performs the training and the\n",
        "validation:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from coco_eval import CocoEvaluator\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = \"Test:\"\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator\n"
      ],
      "metadata": {
        "id": "CFvrZsro3PSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start training our model!"
      ],
      "metadata": {
        "id": "QzpS204I3hvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czdga37xZcEG"
      },
      "outputs": [],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and tank\n",
        "num_classes = 2\n",
        "# get the model using our helper function\n",
        "model = get_model_detection(num_classes)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it just for 2 epochs\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcBUgbdZcEG"
      },
      "source": [
        "So after one epoch of training, we obtain a COCO-style mAP \\> 50, and a\n",
        "mask mAP of 65.\n",
        "\n",
        "But what do the predictions look like? Let's take one image in the\n",
        "dataset and verify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4LY3eMAZcEG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "\n",
        "image = read_image(\"data/AerialTankData/valid/150630-vaux-russia-tease_qeayov_jpeg_jpg.rf.0f1530b045fd8550b96d60fce4efa269.jpg\")\n",
        "eval_transform = get_transform(train=False)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = eval_transform(image)\n",
        "    # convert RGBA -> RGB and move to device\n",
        "    x = x[:3, ...].to(device)\n",
        "    predictions = model([x, ])\n",
        "    pred = predictions[0]\n",
        "\n",
        "\n",
        "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "image = image[:3, ...]\n",
        "pred_labels = [f\"tank: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
        "pred_boxes = pred[\"boxes\"].long()\n",
        "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One technique that researchers have discovered works well is to **freeze** most of the weights of the model, and only tune the very last layer. This should make the model faster to train since there are less weights to calculate gradients for."
      ],
      "metadata": {
        "id": "R7j-1peS7c8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_detection(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "### NEW - Freeze all the weights except for the predictor head\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.roi_heads.box_predictor.parameters():\n",
        "    p.requires_grad = True\n",
        "###\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it just for 2 epochs\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "xIou6oWc_EuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did you observe about the layer freezing? What effect did it have on training speed and accuracy?"
      ],
      "metadata": {
        "id": "az6sz0Bn84wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some things to try\n",
        "1. Change the annotation files used for the datasets to use the entire dataset, and increase the number of epochs (time permitting!)\n",
        "2. Change the backbone of the model\n",
        "3. Change some hyperparameters for training (learning rate, epochs, the scheduler)\n",
        "4. Add to the data augmentations if your model is overfitting to the training set"
      ],
      "metadata": {
        "id": "CPmPXnVL8VA-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xME4lTqfZcEG"
      },
      "source": [
        "Wrapping up\n",
        "===========\n",
        "\n",
        "In this tutorial, you have learned how to create your own training\n",
        "pipeline for object detection models on a custom dataset. You also leveraged a Faster R-CNN model pre-trained on COCO\n",
        "train2017 in order to perform transfer learning on this new dataset.\n",
        "\n",
        "For a more complete example, which includes multi-machine / multi-GPU\n",
        "training, check `references/detection/train.py`, which is present in the\n",
        "[torchvision](https://github.com/pytorch/vision) repository.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}